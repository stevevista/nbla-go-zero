// Copyright (c) 2017 Sony Corporation. All Rights Reserved.
// 
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
// 
//     http://www.apache.org/licenses/LICENSE-2.0
// 
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// *WARNING*
// THIS FILE IS AUTO-GENERATED BY CODE GENERATOR.
// PLEASE DO NOT EDIT THIS FILE BY HAND!
// If you want to modify this file, edit following files.
// - build-tools/code_generator/templates/src_nbla_cuda_init_cpp_template.cpp
// - build-tools/code_generator/generator/generate_src_nbla_cuda_init_cpp.py

#include <nbla/init.hpp>
#include <nbla/cuda/init.hpp>
#include <nbla/cuda/cuda.hpp>
#include <nbla/cuda/common.hpp>
#include <nbla/array_registry.hpp>
#include <nbla/function_registry.hpp>
#include <nbla/array/cpu_array.hpp>
#include <nbla/cuda/array/cuda_array.hpp>
#include <nbla/cuda/function/affine.hpp>
#include <nbla/cuda/function/convolution.hpp>
#include <nbla/cuda/function/tanh.hpp>
#include <nbla/cuda/function/relu.hpp>
#include <nbla/cuda/function/softmax.hpp>
#include <nbla/cuda/function/batch_normalization.hpp>
#include <nbla/cuda/function/add2.hpp>
#include <nbla/cuda/function/mul_scalar.hpp>
#include <nbla/garbage_collector.hpp>

#include <nbla/array/cpu_array.hpp>
#include <nbla/array_registry.hpp>
#include <nbla/cuda/array/cuda_array.hpp>
#include <nbla/cuda/cudnn/cudnn.hpp>
#include <nbla/cuda/cudnn/function/convolution.hpp>
#include <nbla/cuda/cudnn/function/tanh.hpp>
#include <nbla/cuda/cudnn/function/relu.hpp>
#include <nbla/cuda/cudnn/function/softmax.hpp>
#include <nbla/cuda/cudnn/function/add2.hpp>
#include <nbla/cuda/cudnn/init.hpp>
#include <nbla/function_registry.hpp>
#include <nbla/init.hpp>

namespace nbla {

void init_cuda() {
  static bool is_initialized = false;
  if (is_initialized)
    return;

  // Init CPU features
  init_cpu();

  // Array registration
  NBLA_REGISTER_ARRAY_CREATOR(CudaArray);
  SingletonManager::get<Cuda>()->register_array_class("CudaArray");
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuArray, CudaArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaArray, CpuArray,
                                   synchronizer_cuda_array_cpu_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuCachedArray, CudaArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaArray, CpuCachedArray,
                                   synchronizer_cuda_array_cpu_array);
  NBLA_REGISTER_ARRAY_CREATOR(CudaCachedArray);
  SingletonManager::get<Cuda>()->register_array_class("CudaCachedArray");
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuArray, CudaCachedArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaCachedArray, CpuArray,
                                   synchronizer_cuda_array_cpu_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CpuCachedArray, CudaCachedArray,
                                   synchronizer_cpu_array_cuda_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaCachedArray, CpuCachedArray,
                                   synchronizer_cuda_array_cpu_array);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaCachedArray, CudaArray,
                                   synchronizer_default);
  NBLA_REGISTER_ARRAY_SYNCHRONIZER(CudaArray, CudaCachedArray,
                                   synchronizer_default);

  // Function registration
  typedef AffineCuda<float> AffineCudaf;
  NBLA_REGISTER_FUNCTION_IMPL(Affine, AffineCudaf, 1, "cuda", "default", int);
  typedef ConvolutionCuda<float> ConvolutionCudaf;
  NBLA_REGISTER_FUNCTION_IMPL(Convolution, ConvolutionCudaf, 1, "cuda", "default", int, const vector<int> &, const vector<int> &, const vector<int> &, int);
  typedef TanhCuda<float> TanhCudaf;
  NBLA_REGISTER_FUNCTION_IMPL(Tanh, TanhCudaf, 1, "cuda", "default");
  typedef ReLUCuda<float> ReLUCudaf;
  NBLA_REGISTER_FUNCTION_IMPL(ReLU, ReLUCudaf, 1, "cuda", "default", bool);

  typedef SoftmaxCuda<float> SoftmaxCudaf;
  NBLA_REGISTER_FUNCTION_IMPL(Softmax, SoftmaxCudaf, 1, "cuda", "default", int);

  typedef BatchNormalizationCuda<float> BatchNormalizationCudaf;
  NBLA_REGISTER_FUNCTION_IMPL(BatchNormalization, BatchNormalizationCudaf, 1, "cuda", "default", int);

  typedef Add2Cuda<float> Add2Cudaf;
  NBLA_REGISTER_FUNCTION_IMPL(Add2, Add2Cudaf, 1, "cuda", "default");

  typedef MulScalarCuda<float> MulScalarCudaf;
  NBLA_REGISTER_FUNCTION_IMPL(MulScalar, MulScalarCudaf, 1, "cuda", "default");


  // Function registration
  typedef ConvolutionCudaCudnn<float> ConvolutionCudaCudnnf;
  NBLA_REGISTER_FUNCTION_IMPL(Convolution, ConvolutionCudaCudnnf, 2, "cuda", "cudnn", int, const vector<int> &, const vector<int> &, const vector<int> &, int);
  typedef TanhCudaCudnn<float> TanhCudaCudnnf;
  NBLA_REGISTER_FUNCTION_IMPL(Tanh, TanhCudaCudnnf, 2, "cuda", "cudnn");
  typedef ReLUCudaCudnn<float> ReLUCudaCudnnf;
  NBLA_REGISTER_FUNCTION_IMPL(ReLU, ReLUCudaCudnnf, 2, "cuda", "cudnn", bool);
  typedef SoftmaxCudaCudnn<float> SoftmaxCudaCudnnf;
  NBLA_REGISTER_FUNCTION_IMPL(Softmax, SoftmaxCudaCudnnf, 2, "cuda", "cudnn", int);
  typedef Add2CudaCudnn<float> Add2CudaCudnnf;
  NBLA_REGISTER_FUNCTION_IMPL(Add2, Add2CudaCudnnf, 2, "cuda", "cudnn");

  is_initialized = true;
}

void clear_cuda_memory_cache() {
  SingletonManager::get<Cuda>()->memcache().clear();
}

/** Get CUDA array classes.
*/
vector<string> cuda_array_classes() {
  return SingletonManager::get<Cuda>()->array_classes();
}

/** Set CUDA array classes
*/
void _cuda_set_array_classes(const vector<string> &a) {
  return SingletonManager::get<Cuda>()->_set_array_classes(a);
}

void cuda_device_synchronize(int device) {
  cuda_set_device(device);
  NBLA_CUDA_CHECK(cudaDeviceSynchronize());
}

int cuda_get_device_count() {
  int count;
  NBLA_CUDA_CHECK(cudaGetDeviceCount(&count));
  return count;
}

}

